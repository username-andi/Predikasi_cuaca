# -*- coding: utf-8 -*-
"""submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jz-VvLUeUC8onmv7_n2wzINwsRV8QieY

## Import Library
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from IPython.display import display
"""## Data loading"""

df = pd.read_csv("weather_classification_data.csv")
df.head()

df.info()

df.describe()

df.isnull().sum()

df.shape

numerical_cols_original = df.select_dtypes(include=['int64', 'float64']).columns
print("Numerical columns for outlier checking:", numerical_cols_original)

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols_original):
    plt.subplot(2, 4, i + 1) # Sesuaikan grid subplot jika jumlah kolom berbeda
    sns.boxplot(y=df[col])
    plt.title(col)
    plt.ylabel('') # Hapus label y default agar tidak tumpang tindih

plt.tight_layout()
plt.show()

print("\nCek Outlier.")

"""## Data cleaning"""

df_cleaned = df.copy()

numerical_cols = df_cleaned.select_dtypes(include=np.number).columns

for col in numerical_cols:
    Q1 = df_cleaned[col].quantile(0.25)
    Q3 = df_cleaned[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]

display(df_cleaned.head())
print(f"Original number of rows: {len(df)}")
print(f"Number of rows after outlier removal: {len(df_cleaned)}")

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols_original):
    plt.subplot(2, 4, i + 1)
    sns.boxplot(y=df_cleaned[col])
    plt.title(col)
    plt.ylabel('')

plt.tight_layout()
plt.show()

print("\nClear Outlier.")

"""# EDA"""

# 1. Histograms for numerical features
numerical_cols = df_cleaned.select_dtypes(include=np.number).columns
df_cleaned[numerical_cols].hist(bins=30, figsize=(15, 10))
plt.tight_layout()
plt.show()

# 2. Correlation matrix heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df_cleaned[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""## Data preparation"""

categorical_cols = df_cleaned.select_dtypes(include='object').columns
df_processed = pd.get_dummies(df_cleaned, columns=categorical_cols, drop_first=True, dummy_na=False)
display(df_processed.head())

"""## Data splitting"""

X = df_processed.drop('Weather Type_Sunny', axis=1)
y = df_processed['Weather Type_Sunny']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

from sklearn.preprocessing import StandardScaler

# Kolom numerik
numerical_cols = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)',
                  'Atmospheric Pressure', 'UV Index', 'Visibility (km)']

# Inisialisasi scaler dan fit ke X_train
scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])

"""## Model training"""

# Inisialisasi model
knn_model = KNeighborsClassifier()
rf_model = RandomForestClassifier(random_state=42)
gbm_model = GradientBoostingClassifier(random_state=42)
svm_model = SVC(probability=True, class_weight='balanced', random_state=42)

# Latih model
knn_model.fit(X_train_scaled, y_train)
rf_model.fit(X_train, y_train)
gbm_model.fit(X_train, y_train)
svm_model.fit(X_train_scaled, y_train)

print("Models trained successfully.")

"""## Model evaluation"""

# Predict on the test set
y_pred_knn = knn_model.predict(X_test_scaled)
y_pred_svm = svm_model.predict(X_test_scaled)
y_pred_rf = rf_model.predict(X_test)
y_pred_gbm = gbm_model.predict(X_test)

# Evaluate models
print("KNN Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_knn):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_knn):.4f}")
print(f"F1-score: {f1_score(y_test, y_pred_knn):.4f}\n")

print("Random Forest Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_rf):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_rf):.4f}")
print(f"F1-score: {f1_score(y_test, y_pred_rf):.4f}\n")

print("Gradient Boosting Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_gbm):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_gbm):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_gbm):.4f}")
print(f"F1-score: {f1_score(y_test, y_pred_gbm):.4f}\n")

print("SVM Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_svm):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_svm):.4f}")
print(f"F1-score: {f1_score(y_test, y_pred_svm):.4f}\n")

models = {
    'KNN': y_pred_knn,
    'Random Forest': y_pred_rf,
    'Gradient Boosting': y_pred_gbm,
    'SVM': y_pred_svm
}

# Plot confusion matrix untuk setiap model
for name, y_pred in models.items():
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.show()

# Dictionary model dan input data
model_dict = {
    'KNN': (knn_model, X_test_scaled),
    'SVM': (svm_model, X_test_scaled),
    'RandomForest': (rf_model, X_test),
    'GradientBoosting': (gbm_model, X_test)
}

# Dictionary untuk menyimpan hasil metrik
metrics = {
    'accuracy': {},
    'precision': {},
    'recall': {},
    'f1_score': {},
    'roc_auc': {}
}

# Hitung metrik untuk tiap model
for name, (model, x_data) in model_dict.items():
    y_pred = model.predict(x_data)
    y_proba = model.predict_proba(x_data)[:, 1] if hasattr(model, "predict_proba") else None

    metrics['accuracy'][name] = accuracy_score(y_test, y_pred)
    metrics['precision'][name] = precision_score(y_test, y_pred, zero_division=0)
    metrics['recall'][name] = recall_score(y_test, y_pred, zero_division=0)
    metrics['f1_score'][name] = f1_score(y_test, y_pred, zero_division=0)
    metrics['roc_auc'][name] = roc_auc_score(y_test, y_proba) if y_proba is not None else None

metrics_df = pd.DataFrame(metrics).T
print(metrics_df.round(6))

"""## Inferensi"""

from sklearn.preprocessing import LabelEncoder

# Asumsikan LabelEncoder sudah fit di data training
le = LabelEncoder()
le.fit(df['Weather Type'])

# Data baru (contoh) tanpa label 'Weather Type'
test_samples = [
    [23.0, 83, 1.5, 82.0, 1010.82, 2, 3.5, 'clear', 'Spring', 'inland'],
    [39.0, 96, 8.5, 71.0, 1011.43, 7, 10.0, 'partly cloudy', 'Summer', 'mountain'],
    [30.0, 64, 7.0, 16.0, 1018.72, 5, 5.5, 'cloudy', 'Summer', 'inland'],
]

columns = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)', 'Atmospheric Pressure',
           'UV Index', 'Visibility (km)', 'Cloud Cover', 'Season', 'Location']

df_new = pd.DataFrame(test_samples, columns=columns)

# Proses one-hot encoding sesuai X_train
X_new_dummies = pd.get_dummies(df_new)

# Tambahkan kolom dummy yang hilang agar kolom sesuai dengan X_train
missing_cols = set(X_train.columns) - set(X_new_dummies.columns)
for col in missing_cols:
    X_new_dummies[col] = 0

# Urutkan kolom agar sama persis dengan X_train
X_new_dummies = X_new_dummies[X_train.columns]

# Skala fitur numerik untuk model KNN dan SVM
numerical_cols = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)',
                  'Atmospheric Pressure', 'UV Index', 'Visibility (km)']

X_new_dummies_scaled = X_new_dummies.copy()
X_new_dummies_scaled[numerical_cols] = scaler.transform(X_new_dummies[numerical_cols])

# Dictionary model yang sudah dilatih
model_dict = {
    'KNN': knn_model,
    'SVM': svm_model,
    'Random Forest': rf_model,
    'Gradient Boosting': gbm_model
}

# Map model ke input yang sesuai (scaled atau tidak)
model_input_map = {
    'KNN': X_new_dummies_scaled,
    'SVM': X_new_dummies_scaled,
    'Random Forest': X_new_dummies,
    'Gradient Boosting': X_new_dummies
}

# Fungsi prediksi dengan output label string
def predict_with_labels(model, input_df, label_encoder):
    preds = model.predict(input_df)
    labels = label_encoder.inverse_transform(preds)
    return labels

# Prediksi dan cetak hasil untuk semua model
for name, model in model_dict.items():
    input_data = model_input_map[name]
    preds_labels = predict_with_labels(model, input_data, le)
    print(f"Prediksi oleh {name}: {list(preds_labels)}")

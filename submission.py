# -*- coding: utf-8 -*-
"""submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qDgByFByEa7ReH-a0ckm69-a9XNIjzLU

## Import Library

Dilakukan proses import library yang dibutuhkan untuk menunjang seluruh tahapan machine learning. NumPy dan Pandas merupakan dua library utama yang digunakan untuk manipulasi data numerik dan tabular. Untuk keperluan visualisasi data, digunakan Matplotlib.pyplot sebagai pustaka visualisasi dasar yang fleksibel dan banyak digunakan. Seaborn, yang merupakan library visualisasi berbasis Matplotlib dengan tampilan yang lebih estetis, digunakan untuk membuat grafik seperti heatmap, boxplot, dan pairplot. Modul dari scikit-learn digunakan untuk pembentukan model dan evaluasi.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from IPython.display import display

"""## Data loading

Membaca file dataset
"""

df = pd.read_csv("weather_classification_data.csv")
df.head()

"""Perintah df.info() digunakan untuk menampilkan informasi umum tentang struktur DataFrame, seperti jumlah baris dan kolom, nama kolom, tipe data di setiap kolom, serta jumlah nilai non-null (tidak hilang) pada masing-masing kolom."""

df.info()

"""Perintah df.describe() digunakan untuk menampilkan statistik deskriptif dari kolom numerik dalam DataFrame, seperti mean, standar deviasi, nilai minimum, maksimum, serta kuartil (25%, 50%, 75%), yang membantu memahami sebaran data secara umu"""

df.describe()

"""Kode dibawah untuk mengecek apakah ada data yang kosong perkolom"""

df.isnull().sum()

"""Kode dibawah untuk mengecek baris dan kolom dalam dataset"""

df.shape

"""Kode dibawah untuk melihat kolom yang bertipe numerik"""

numerical_cols_original = df.select_dtypes(include=['int64', 'float64']).columns
print("Numerical columns for outlier checking:", numerical_cols_original)

"""Kode dibawah merupakan kode untuk cek outlier"""

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols_original):
    plt.subplot(2, 4, i + 1)
    sns.boxplot(y=df[col])
    plt.title(col)
    plt.ylabel('')

plt.tight_layout()
plt.show()

print("\nCek Outlier.")

"""Insight: Berdasarkan langkah di atas, kita dapat melihat berbagai informasi terkait data tersebut. Data tersebut terdiri dari 13.200 baris dan 11 kolom, tidak terdapat missing value dan terdapat 3 tipe data (float, int, object). Setelah melakukan visualisasi fitur data numerik didapatkan informasi bahwa data tersebut terdapat outlier.

## Data cleaning

Dilakukan penanganan outlier dengan menggunakan metode IQR(Interquartile Range).
"""

df_cleaned = df.copy()

numerical_cols = df_cleaned.select_dtypes(include=np.number).columns

for col in numerical_cols:
    Q1 = df_cleaned[col].quantile(0.25)
    Q3 = df_cleaned[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]

display(df_cleaned.head())
print(f"Original number of rows: {len(df)}")
print(f"Number of rows after outlier removal: {len(df_cleaned)}")

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols_original):
    plt.subplot(2, 4, i + 1)
    sns.boxplot(y=df_cleaned[col])
    plt.title(col)
    plt.ylabel('')

plt.tight_layout()
plt.show()

print("\nClear Outlier.")

"""# EDA

Pada tahap eksplorasi data ini, dilakukan analisis terhadap fitur-fitur numerik dalam dataset menggunakan visualisasi histogram dan matriks korelasi (heatmap). Visualisasi histogram bertujuan untuk memahami distribusi masing-masing fitur numerik seperti suhu (Temperature), kelembapan (Humidity), kecepatan angin (Wind Speed), curah hujan (Precipitation), tekanan atmosfer (Atmospheric Pressure), indeks UV (UV Index), dan jarak pandang (Visibility).

Setiap histogram menampilkan sebaran nilai dalam bentuk batang, memungkinkan kita mengidentifikasi apakah data bersifat normal, skewed (miring), atau memiliki outlier. Sebagai contoh, fitur Wind Speed dan Visibility menunjukkan distribusi yang sedikit miring ke kanan, sementara fitur seperti Humidity dan Temperature cenderung mengikuti distribusi normal.

Selanjutnya, digunakan visualisasi heatmap korelasi untuk melihat hubungan linier antar fitur numerik. Korelasi dihitung menggunakan koefisien Pearson, dan divisualisasikan menggunakan fungsi heatmap dari Seaborn. Warna merah menunjukkan korelasi positif kuat, sedangkan warna biru menunjukkan korelasi negatif.
"""

numerical_cols = df_cleaned.select_dtypes(include=np.number).columns
df_cleaned[numerical_cols].hist(bins=30, figsize=(15, 10))
plt.tight_layout()
plt.show()

"""Insight:

Berdasarkan hasil eksplorasi data melalui visualisasi histogram dan matriks korelasi, terdapat beberapa temuan penting terkait karakteristik dan hubungan antar fitur numerik dalam dataset cuaca ini. Pertama, distribusi suhu (temperature) menunjukkan pola bimodal, yang mengindikasikan kemungkinan adanya dua kelompok data yang berbeda, seperti perbedaan musim atau wilayah geografis (misalnya dataran tinggi dan rendah). Kelembapan (humidity) mayoritas berada pada rentang tinggi antara 60% hingga 100%, menandakan kondisi udara yang cenderung lembap. Kecepatan angin (wind speed) didominasi pada rentang 5–15 km/jam tanpa banyak nilai ekstrem, sedangkan curah hujan (precipitation) menunjukkan distribusi yang menyebar dengan beberapa puncak, mencerminkan variasi kondisi cuaca dari kering hingga hujan lebat.

Tekanan atmosfer (atmospheric pressure) terdistribusi mendekati normal dengan pusat di sekitar 1010–1020 hPa, mencerminkan kestabilan atmosfer. Indeks UV (UV index) umumnya berada pada nilai rendah (0–2), yang bisa menandakan pengambilan data lebih sering terjadi saat cuaca mendung atau di wilayah dengan paparan sinar matahari rendah. Sementara itu, visibilitas (visibility) terbagi ke dalam dua kelompok besar, yaitu visibilitas tinggi dan rendah, yang kemungkinan dipengaruhi oleh kondisi cuaca seperti kabut atau hujan.
"""

plt.figure(figsize=(12, 8))
sns.heatmap(df_cleaned[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""Insight:

Dari matriks korelasi, terlihat bahwa kelembapan memiliki korelasi positif yang kuat dengan curah hujan (0.71), sesuai dengan logika bahwa kelembapan tinggi meningkatkan kemungkinan hujan. Curah hujan juga memiliki korelasi negatif kuat dengan visibilitas (-0.65), menandakan bahwa hujan menurunkan jarak pandang. Selain itu, tekanan atmosfer memiliki korelasi positif sedang dengan suhu (0.68), indeks UV (0.54), dan visibilitas (0.56), yang menunjukkan bahwa kondisi tekanan tinggi umumnya diasosiasikan dengan cuaca cerah dan jarak pandang yang lebih baik. Sebaliknya, suhu dan kelembapan memiliki korelasi negatif sedang (-0.33), mencerminkan bahwa suhu tinggi sering kali diikuti dengan kelembapan yang lebih rendah.

## Data preparation

Merubah kolom bertipe kategorial (object) menjadi format numerik agar dapat digunakan dalam model machine learning dengan menggunakan metode one-hot encoding.
"""

categorical_cols = df_cleaned.select_dtypes(include='object').columns
df_processed = pd.get_dummies(df_cleaned, columns=categorical_cols, drop_first=True, dummy_na=False)
display(df_processed.head())

"""## Data splitting

Dataset dibagi menjadi data latih (train) dan data uji (test) menggunakan train_test_split dari sklearn dengan rasio 80:20. Hal ini dilakukan untuk memisahkan data pada proses pelatihan dan evaluasi model.
"""

X = df_processed.drop('Weather Type_Sunny', axis=1)
y = df_processed['Weather Type_Sunny']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""Scaling dilakukan menggunakan StandardScaler agar setiap fitur memiliki mean = 0 dan standar deviasi = 1 untuk model berbasis jarak seperti KNN dan SVM."""

from sklearn.preprocessing import StandardScaler

numerical_cols = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)',
                  'Atmospheric Pressure', 'UV Index', 'Visibility (km)']

scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])

"""## Model training

Tahap selanjutnya adalah pemodelan menggunakan empat algoritma klasifikasi: K-Nearest Neighbors (KNN), Random Forest, Gradient Boosting, dan Support Vector Machine (SVM). Pada kode ini, beberapa model seperti KNN dan SVM dibangun menggunakan data yang telah distandarisasi (dengan StandardScaler), sementara Random Forest dan Gradient Boosting menggunakan data asli tanpa skalasi, karena kedua algoritma tersebut tidak terlalu sensitif terhadap skala fitur. Setiap model dilatih (fit) pada data pelatihan yang sesuai.
"""

knn_model = KNeighborsClassifier()
rf_model = RandomForestClassifier(random_state=42)
gbm_model = GradientBoostingClassifier(random_state=42)
svm_model = SVC(probability=True, class_weight='balanced', random_state=42)

knn_model.fit(X_train_scaled, y_train)
rf_model.fit(X_train, y_train)
gbm_model.fit(X_train, y_train)
svm_model.fit(X_train_scaled, y_train)

print("Models trained successfully.")

"""## Model evaluation

Setelah model selesai dilatih, langkah selanjutnya adalah melakukan evaluasi performa untuk menilai kemampuan masing-masing model dalam memprediksi kualitas udara. Proses evaluasi ini dilakukan secara konsisten pada semua model, yaitu K-Nearest Neighbors (KNN), Random Forest, Gradient Boosting, dan Support Vector Machine (SVM).

Evaluasi dilakukan dengan menghitung metrik-metrik penting seperti accuracy, precision, recall, dan F1-score yang memberikan gambaran umum tentang performa model. Selain itu, digunakan juga confusion matrix yang divisualisasikan untuk melihat distribusi prediksi benar dan salah pada setiap kelas target, sehingga dapat dianalisis lebih mendalam.
"""

y_pred_knn = knn_model.predict(X_test_scaled)
y_pred_svm = svm_model.predict(X_test_scaled)
y_pred_rf = rf_model.predict(X_test)
y_pred_gbm = gbm_model.predict(X_test)

print("KNN Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_knn):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_knn):.4f}")
print(f"F1-score: {f1_score(y_test, y_pred_knn):.4f}\n")

print("Random Forest Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_rf):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_rf):.4f}")
print(f"F1-score: {f1_score(y_test, y_pred_rf):.4f}\n")

print("Gradient Boosting Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_gbm):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_gbm):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_gbm):.4f}")
print(f"F1-score: {f1_score(y_test, y_pred_gbm):.4f}\n")

print("SVM Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_svm):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_svm):.4f}")
print(f"F1-score: {f1_score(y_test, y_pred_svm):.4f}\n")

models = {
    'KNN': y_pred_knn,
    'Random Forest': y_pred_rf,
    'Gradient Boosting': y_pred_gbm,
    'SVM': y_pred_svm
}

for name, y_pred in models.items():
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.show()

model_dict = {
    'KNN': (knn_model, X_test_scaled),
    'SVM': (svm_model, X_test_scaled),
    'RandomForest': (rf_model, X_test),
    'GradientBoosting': (gbm_model, X_test)
}

metrics = {
    'accuracy': {},
    'precision': {},
    'recall': {},
    'f1_score': {},
    'roc_auc': {}
}

for name, (model, x_data) in model_dict.items():
    y_pred = model.predict(x_data)
    y_proba = model.predict_proba(x_data)[:, 1] if hasattr(model, "predict_proba") else None

    metrics['accuracy'][name] = accuracy_score(y_test, y_pred)
    metrics['precision'][name] = precision_score(y_test, y_pred, zero_division=0)
    metrics['recall'][name] = recall_score(y_test, y_pred, zero_division=0)
    metrics['f1_score'][name] = f1_score(y_test, y_pred, zero_division=0)
    metrics['roc_auc'][name] = roc_auc_score(y_test, y_proba) if y_proba is not None else None

metrics_df = pd.DataFrame(metrics).T
print(metrics_df.round(6))

"""## Inferensi

Setelah model dievaluasi dan menunjukkan performa yang baik, dilakukan tahap inferensi untuk menguji kemampuan model dalam memprediksi cuaca pada data baru.
"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(df['Weather Type'])

test_samples = [
    [23.0, 83, 1.5, 82.0, 1010.82, 2, 3.5, 'clear', 'Spring', 'inland'],
    [39.0, 96, 8.5, 71.0, 1011.43, 7, 10.0, 'partly cloudy', 'Summer', 'mountain'],
    [30.0, 64, 7.0, 16.0, 1018.72, 5, 5.5, 'cloudy', 'Summer', 'inland'],
]

columns = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)', 'Atmospheric Pressure',
           'UV Index', 'Visibility (km)', 'Cloud Cover', 'Season', 'Location']

df_new = pd.DataFrame(test_samples, columns=columns)

X_new_dummies = pd.get_dummies(df_new)

missing_cols = set(X_train.columns) - set(X_new_dummies.columns)
for col in missing_cols:
    X_new_dummies[col] = 0

X_new_dummies = X_new_dummies[X_train.columns]

numerical_cols = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)',
                  'Atmospheric Pressure', 'UV Index', 'Visibility (km)']

X_new_dummies_scaled = X_new_dummies.copy()
X_new_dummies_scaled[numerical_cols] = scaler.transform(X_new_dummies[numerical_cols])

model_dict = {
    'KNN': knn_model,
    'SVM': svm_model,
    'Random Forest': rf_model,
    'Gradient Boosting': gbm_model
}

model_input_map = {
    'KNN': X_new_dummies_scaled,
    'SVM': X_new_dummies_scaled,
    'Random Forest': X_new_dummies,
    'Gradient Boosting': X_new_dummies
}

def predict_with_labels(model, input_df, label_encoder):
    preds = model.predict(input_df)
    labels = label_encoder.inverse_transform(preds)
    return labels

for name, model in model_dict.items():
    input_data = model_input_map[name]
    preds_labels = predict_with_labels(model, input_data, le)
    print(f"Prediksi oleh {name}: {list(preds_labels)}")